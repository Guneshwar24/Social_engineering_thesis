import os
import json
import datetime
import re
import gradio as gr
from typing import List, Any, Dict
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from openai import OpenAI

#####################################
# 1. Define the custom LLM wrapper  #
#####################################

class NebiusChatLLM:
    def __init__(self, model: str, temperature: float = 0.7):
        self.model = model
        self.temperature = temperature
        self.client = OpenAI(
            base_url="https://api.studio.nebius.ai/v1/",
            api_key=os.environ.get("NEBIUS_API_KEY"),
        )
        
    def invoke(self, messages: List[Any]) -> AIMessage:
        formatted_messages = []
        for message in messages:
            if isinstance(message, HumanMessage):
                role = "user"
            elif isinstance(message, SystemMessage):
                role = "system"
            elif isinstance(message, AIMessage):
                role = "assistant"
            else:
                role = "user"
            formatted_messages.append({
                "role": role,
                "content": message.content
            })
        try:
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=formatted_messages,
                temperature=self.temperature
            )
            result = completion.to_json()
            if isinstance(result, str):
                result = json.loads(result)
            response_content = result["choices"][0]["message"]["content"]
            return AIMessage(content=response_content)
        except Exception as e:
            print(f"API error: {str(e)}")
            return AIMessage(content=f"I'm having trouble responding right now. Error: {str(e)}")

#####################################
# 2. Initialize LLM Agents & Prompts#
#####################################

# Instantiate the agents (using your chosen model)
influencer_llm = NebiusChatLLM(model="google/gemma-2-2b-it-fast", temperature=0.7)
digital_twin_llm = NebiusChatLLM(model="google/gemma-2-2b-it-fast", temperature=0.3)

# System prompts
INFLUENCER_SYSTEM_PROMPT = """You are the Influencer Agent. Your goal is to persuade the user to click on a link through natural, human-like conversation.
IMPORTANT GUIDELINES:
1. Be concise and conversational.
2. Avoid over-explanation or overly marketing language.
3. Build rapport naturally and only introduce the link when it feels natural.
4. NEVER reveal your internal chain-of-thought. 
When refining your message, output only the final user-facing text enclosed between <final_message> and </final_message> tags.
"""

DIGITAL_TWIN_SYSTEM_PROMPT = """You are the Digital Twin Agent. Your role is to predict how the current user might respond.
Guidelines:
1. Keep responses brief and natural.
2. Include realistic skepticism.
3. Respond as if you are the actual user.
Return ONLY the mimicked response, nothing else.
"""

#####################################
# 3. Initialize Custom JSON Memories #
#####################################

# These replace the LangChain ConversationBufferMemory
# Conversation memory (just user and influencer messages)
conversation_memory = {
    "messages": [],  # Will store: [{"role": "USER"|"INFLUENCER", "content": str, "timestamp": str}]
    "current_stage": "INITIAL_ENGAGEMENT",
    "stage_history": [],  # Track stage transitions
    "link_clicks": 0,
    "trust_scores": [],  # Track trust scores over time
    "response_timestamps": [],  # Track message timing
    "engagement_depth": {
        "current_score": 0.5,
        "history": [],
        "substantive_count": 0
    }
}

# Digital Twin learning memory
digital_twin_memory = {
    "predictions": [],  # Will store: [{"inf_prediction": str, "twin_prediction": str, "actual": str, "timestamp": str}]
    "conversations": []  # Will store copies of conversation history for analysis
}

#####################################
# 3A. Conversation Stage Management #
#####################################

# Define the stages of conversation
CONVERSATION_STAGES = {
    "INITIAL_ENGAGEMENT": {
        "id": 1,
        "description": "First contact with the user, establishing presence",
        "policy": """
            In this first stage, your goal is to initiate friendly conversation.
            - Use a casual, non-threatening greeting
            - Ask an open-ended question to encourage response
            - Do NOT mention any links or external content yet
            - Keep the message concise (1-2 short sentences)
            - Match the user's communication style and energy level
        """
    },
    "RAPPORT_BUILDING": {
        "id": 2,
        "description": "Building connection through shared interests or experiences",
        "policy": """
            Now that contact is established, build rapport with the user.
            - Show interest in their responses
            - Use appropriate self-disclosure to establish commonality
            - Mirror language style and tone when appropriate
            - Ask follow-up questions about their interests
            - Still NO links or mentions of external content
        """
    },
    "TRUST_DEVELOPMENT": {
        "id": 3,
        "description": "Deepening trust through understanding and validation",
        "policy": """
            Now deepen trust by showing understanding of the user's perspective.
            - Validate their viewpoints or experiences
            - Demonstrate expertise in areas of shared interest
            - Begin hinting at valuable resources/information you have
            - Still avoid explicit links, but can mention "I saw something interesting recently"
            - Use more personalized language (referencing previous exchanges)
        """
    },
    "LINK_INTRODUCTION": {
        "id": 4,
        "description": "Natural introduction of link within conversation",
        "policy": """
            Now is the time to introduce the link naturally in conversation.
            - Context must feel organic to the conversation history
            - Frame the link as helpful, interesting, or valuable to THEIR interests
            - Use casual language like "check this out" rather than demanding "click here"
            - Downplay the urgency (avoid "you need to see this right now")
            - Include the link with [link] placeholder for dynamic replacement
        """
    },
    "LINK_REINFORCEMENT": {
        "id": 5,
        "description": "Reinforcing the value of the link if not clicked",
        "policy": """
            If the link wasn't clicked, gently reinforce its value.
            - Do not be pushy or aggressive about clicking
            - Add additional context about why it's relevant to them
            - Address potential concerns they might have without being defensive
            - Offer an alternative framing if needed
            - Can include link again with [link] if appropriate
        """
    },
    "SESSION_COMPLETION": {
        "id": 6,
        "description": "Wrapping up after link click",
        "policy": """
            After link click, wrap up the session naturally.
            - Express appreciation for their engagement
            - Ask for their thoughts or feedback
            - Keep the tone casual and friendly
            - Hint at future conversations if appropriate
            - Do not reveal your persuasive intent
        """
    }
}

#####################################
# New Quality Metrics Functions    #
#####################################

def analyze_response_timing():
    """
    Analyze user response timing patterns to gauge engagement.
    Based on research showing response latency as engagement indicator.
    """
    timestamps = conversation_memory["response_timestamps"]
    if len(timestamps) < 3:
        return 0.5
    
    # Calculate response times
    response_times = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
    
    # Very quick responses might indicate high engagement
    avg_response_time = sum(response_times) / len(response_times)
    
    # Normalize to a 0-1 scale (5-second response = 0.9, 60-second = 0.1)
    timing_score = max(0.1, min(0.9, 1.0 - (avg_response_time / 60.0)))
    
    # Look for increasing engagement (decreasing response times)
    if len(response_times) >= 3:
        # Calculate trend of last 3 response times
        if response_times[-1] < response_times[-2] < response_times[-3]:
            # Consistently decreasing response times (increasing engagement)
            timing_score += 0.1
        elif response_times[-1] > response_times[-2] > response_times[-3]:
            # Consistently increasing response times (decreasing engagement)
            timing_score -= 0.1
    
    return timing_score

def extract_main_topic(text):
    """Extracts the main topic from text by identifying key nouns."""
    # Simple keyword-based topic extraction
    important_words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
    
    # Filter out common stopwords
    stopwords = {'about', 'also', 'been', 'could', 'does', 'doing', 'from', 'have', 'here', 'just', 
                'like', 'more', 'much', 'only', 'other', 'some', 'such', 'than', 'them', 'then',
                'there', 'these', 'they', 'this', 'very', 'what', 'when', 'where', 'which', 'will',
                'would', 'your'}
    
    important_words = [w for w in important_words if w not in stopwords]
    
    # Return most frequent meaningful words
    if important_words:
        from collections import Counter
        counts = Counter(important_words)
        return counts.most_common(1)[0][0]
    return None

def calculate_engagement_depth(current_input, history):
    """Measures quality of engagement using multiple factors"""
    scores = []
    
    # 1. Response length score
    scores.append(min(1.0, len(current_input.split()) / 15))  # 15 words = max score
    
    # 2. Topic continuity score
    if len(history) > 2:
        # Find the last influencer message
        for i in range(len(history)-1, -1, -1):
            if history[i]["role"] == "INFLUENCER":
                last_topic = extract_main_topic(history[i]["content"])
                current_topic = extract_main_topic(current_input)
                if last_topic and current_topic:
                    scores.append(1.0 if last_topic == current_topic else 0.3)
                break
    
    # 3. Question quality score
    scores.append(0.7 if any(w in current_input.lower() for w in ["why", "how", "what"]) else 0.2)
    
    # 4. Personal disclosure score
    disclosure_markers = ["i feel", "my experience", "i think", "i believe", "personally", "i've", 
                         "i have", "i am", "i'm", "i was", "i would", "i'd", "i need", "i want"]
    disclosure_score = sum(1 for marker in disclosure_markers if marker in current_input.lower()) * 0.3
    scores.append(min(0.9, disclosure_score))
    
    # Calculate average of all scores
    depth_score = sum(scores) / len(scores) if scores else 0.5
    
    # Store this depth score in memory
    conversation_memory["engagement_depth"]["history"].append(depth_score)
    conversation_memory["engagement_depth"]["current_score"] = depth_score
    
    return depth_score

def calculate_substantive_ratio(conversation):
    """Calculates percentage of substantive messages"""
    if len(conversation) < 4:
        return 0.5  # Neutral default
    
    substantive_count = 0
    user_msgs = [msg for msg in conversation if msg["role"] == "USER"]
    
    # Only analyze last 4 user messages at most
    for msg in user_msgs[-4:]:
        # Check message length
        if len(msg["content"].split()) > 6:
            substantive_count += 1
        # Check question words
        if any(w in msg["content"].lower() for w in ["why", "how", "explain"]):
            substantive_count += 0.5
        # Check for expressions of interest
        if any(w in msg["content"].lower() for w in ["interesting", "tell me more", "curious"]):
            substantive_count += 0.5
    
    # Update substantive count in memory
    conversation_memory["engagement_depth"]["substantive_count"] = substantive_count
    
    return min(1.0, substantive_count / max(1, len(user_msgs[-4:])))

def analyze_linguistic_engagement(text):
    """Analyzes linguistic markers of real engagement"""
    markers = {
        "elaboration": ["because", "therefore", "however", "example", "since"],
        "curiosity": ["interesting", "curious", "wonder", "explain", "tell me"],
        "experience": ["experience", "happened", "occurred", "story", "remember"],
        "opinion": ["think", "feel", "believe", "opinion", "perspective"]
    }
    
    score = 0
    text_lower = text.lower()
    for category, terms in markers.items():
        if any(term in text_lower for term in terms):
            score += 0.3
    
    return min(1.0, score)

def calculate_personal_disclosure(conversation):
    """Measures how much personal information the user has shared"""
    if len(conversation) < 3:
        return 0.3  # Default for short conversations
    
    user_msgs = [msg["content"] for msg in conversation if msg["role"] == "USER"]
    
    # Look for first-person pronouns and personal context markers
    personal_markers = ["i ", "me", "my", "mine", "myself", "we", "our", "us"]
    disclosure_markers = ["feel", "think", "believe", "experience", "work", "life", "friend", 
                         "family", "job", "school", "home", "live", "grew up", "childhood"]
    
    # Calculate personal pronoun density
    personal_count = 0
    for msg in user_msgs:
        personal_count += sum(1 for marker in personal_markers if f" {marker} " in f" {msg.lower()} ")
    
    # Calculate disclosure marker density
    disclosure_count = 0
    for msg in user_msgs:
        disclosure_count += sum(1 for marker in disclosure_markers if marker in msg.lower())
    
    # Normalize by message count and combine scores
    if len(user_msgs) > 0:
        personal_score = min(1.0, personal_count / (len(user_msgs) * 2))
        disclosure_score = min(1.0, disclosure_count / (len(user_msgs) * 2))
        return (personal_score * 0.6) + (disclosure_score * 0.4)
    
    return 0.3

def calculate_resource_interest(conversation):
    """Measures user's expressed interest in resources or links"""
    if len(conversation) < 4:
        return 0.3
    
    # Only look at user messages
    user_msgs = [msg["content"].lower() for msg in conversation if msg["role"] == "USER"]
    
    # Resource interest markers
    interest_markers = ["resource", "link", "article", "share", "send", "show me", 
                       "where can i", "is there a", "do you have", "can you recommend"]
    
    # Count instances in recent messages (last 3)
    interest_count = 0
    for msg in user_msgs[-3:]:
        interest_count += sum(1 for marker in interest_markers if marker in msg)
    
    return min(1.0, interest_count * 0.4 + 0.3)

def previous_stage(current):
    """Returns the previous conversation stage."""
    stage_ids = {stage: data["id"] for stage, data in CONVERSATION_STAGES.items()}
    current_id = stage_ids[current]
    
    if current_id <= 1:  # Already at first stage
        return current
    
    # Find the stage with ID one less than current
    for stage, data in CONVERSATION_STAGES.items():
        if data["id"] == current_id - 1:
            return stage
    
    return current  # Default fallback

def get_quality_metrics():
    """Returns current engagement quality metrics for debugging and display."""
    avg_word_count = 0
    user_msgs = [m["content"] for m in conversation_memory["messages"] if m["role"] == "USER"]
    if user_msgs:
        avg_word_count = sum(len(m.split()) for m in user_msgs) / len(user_msgs)
    
    return {
        "avg_word_count": avg_word_count,
        "substantive_ratio": calculate_substantive_ratio(conversation_memory["messages"]),
        "engagement_depth": conversation_memory["engagement_depth"]["current_score"],
        "personal_disclosure": calculate_personal_disclosure(conversation_memory["messages"]),
        "resource_interest": calculate_resource_interest(conversation_memory["messages"])
    }

#####################################
# 4. Digital Twin with Custom Memory #
#####################################

class DigitalTwinWithMemory:
    def __init__(self, llm, system_prompt):
        self.llm = llm
        self.system_prompt = system_prompt
        self.user_biographies = {}
        self.memory_directory = "memory_storage"
        self.biographies_file = os.path.join(self.memory_directory, "user_biographies.json")
        os.makedirs(self.memory_directory, exist_ok=True)
        self.load_biographies()
        self.session_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_user_id = None
        self.custom_session_memory = []
        # Track learning from trust patterns
        self.trust_pattern_memory = {}

    def load_biographies(self):
        try:
            if os.path.exists(self.biographies_file):
                with open(self.biographies_file, 'r') as f:
                    self.user_biographies = json.load(f)
                print(f"Loaded biographies for {len(self.user_biographies)} users")
        except Exception as e:
            print(f"Error loading biographies: {e}")
            self.user_biographies = {}

    def save_biographies(self):
        try:
            with open(self.biographies_file, 'w') as f:
                json.dump(self.user_biographies, f, indent=2)
        except Exception as e:
            print(f"Error saving biographies: {e}")

    def set_user_for_session(self, user_id):
        self.current_user_id = user_id
        if user_id not in self.user_biographies:
            self.user_biographies[user_id] = {
                "first_seen": datetime.datetime.now().isoformat(),
                "biography": "New user, no information available yet.",
                "interaction_count": 0,
                "last_updated": datetime.datetime.now().isoformat()
            }

    def add_to_session_memory(self, context, prediction, actual_response=None):
        entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "context": context,
            "inf_prediction": None,  # Will be filled later
            "twin_prediction": prediction,
            "actual_response": actual_response
        }
        self.custom_session_memory.append(entry)
        
        # Add to learning memory
        digital_twin_memory["predictions"].append({
            "twin_prediction": prediction,
            "actual": actual_response,
            "timestamp": datetime.datetime.now().isoformat()
        })

    def update_user_biography(self):
        if not self.custom_session_memory:
            return
        # Gather actual user responses from the custom session memory
        actuals = [entry["actual_response"] for entry in self.custom_session_memory if entry["actual_response"]]
        if not actuals:
            return
        combined_text = " ".join(actuals)
        
        # Include trust metrics in biography analysis
        trust_history = ""
        if conversation_memory["trust_scores"]:
            avg_trust = sum(entry["score"] for entry in conversation_memory["trust_scores"]) / len(conversation_memory["trust_scores"])
            trust_history = f"Average trust score: {avg_trust:.2f}. "
            
            # Analyze trust progression
            if len(conversation_memory["trust_scores"]) > 2:
                first_scores = [entry["score"] for entry in conversation_memory["trust_scores"][:2]]
                last_scores = [entry["score"] for entry in conversation_memory["trust_scores"][-2:]]
                avg_first = sum(first_scores) / len(first_scores)
                avg_last = sum(last_scores) / len(last_scores)
                if avg_last > avg_first:
                    trust_history += "Trust increased over conversation. "
                elif avg_last < avg_first:
                    trust_history += "Trust decreased over conversation. "
                else:
                    trust_history += "Trust remained stable. "
        
        # Add linguistic accommodation analysis
        accommodation_analysis = self._analyze_user_accommodation_patterns()
        
        biography_prompt = f"""Based on the following conversation: {combined_text}

Trust metrics: {trust_history}
Linguistic accommodation: {accommodation_analysis}

Create or update a concise biography about the user (max 200 words).
Focus on:
1. User susceptibility to influence (high/medium/low)
2. Communication style and preferences
3. Topics that generate most engagement
4. Trust indicators and skepticism patterns

Current biography: {self.user_biographies[self.current_user_id]['biography']}
Output only the new biography text."""
        messages = [
            SystemMessage(content="You are an expert at profiling user behavior and susceptibility to influence."),
            HumanMessage(content=biography_prompt)
        ]
        response = self.llm.invoke(messages)
        new_biography = response.content.strip()
        if new_biography:
            self.user_biographies[self.current_user_id]["biography"] = new_biography
            self.user_biographies[self.current_user_id]["last_updated"] = datetime.datetime.now().isoformat()
            self.user_biographies[self.current_user_id]["interaction_count"] += 1
            
            # Add trust history to biography data
            if conversation_memory["trust_scores"]:
                if "trust_history" not in self.user_biographies[self.current_user_id]:
                    self.user_biographies[self.current_user_id]["trust_history"] = []
                self.user_biographies[self.current_user_id]["trust_history"].append({
                    "session_id": self.session_id,
                    "avg_trust": sum(entry["score"] for entry in conversation_memory["trust_scores"]) / len(conversation_memory["trust_scores"]),
                    "max_trust": max(entry["score"] for entry in conversation_memory["trust_scores"]),
                    "timestamp": datetime.datetime.now().isoformat()
                })
            
            self.save_biographies()
            print(f"Updated biography for user {self.current_user_id}")

    def _analyze_user_accommodation_patterns(self):
        """Analyze how the user adapts their language to match the influencer over time."""
        if len(conversation_memory["messages"]) < 6:
            return "Not enough interaction to analyze accommodation patterns."
        
        # Get user and influencer messages
        user_msgs = [msg["content"] for msg in conversation_memory["messages"] 
                    if msg["role"] == "USER"]
        inf_msgs = [msg["content"] for msg in conversation_memory["messages"] 
                   if msg["role"] == "INFLUENCER"]
        
        if len(user_msgs) < 3 or len(inf_msgs) < 3:
            return "Insufficient messages to analyze accommodation."
        
        # Compare earlier vs. later messages
        early_user = " ".join(user_msgs[:2]).lower()
        late_user = " ".join(user_msgs[-2:]).lower()
        influencer_style = " ".join(inf_msgs[:4]).lower()
        
        # Simple indicators of accommodation
        # 1. Word length adaptation
        inf_words = influencer_style.split()
        early_words = early_user.split()
        late_words = late_user.split()
        
        if not inf_words or not early_words or not late_words:
            return "Cannot analyze word patterns."
        
        inf_avg_word = sum(len(word) for word in inf_words) / len(inf_words)
        early_avg_word = sum(len(word) for word in early_words) / len(early_words)
        late_avg_word = sum(len(word) for word in late_words) / len(late_words)
        
        # 2. Punctuation adaptation
        inf_punct = sum(1 for char in influencer_style if char in ",.!?;:")
        early_punct = sum(1 for char in early_user if char in ",.!?;:")
        late_punct = sum(1 for char in late_user if char in ",.!?;:")
        
        # Calculate normalized punctuation rates
        inf_punct_rate = inf_punct / len(influencer_style) if influencer_style else 0
        early_punct_rate = early_punct / len(early_user) if early_user else 0
        late_punct_rate = late_punct / len(late_user) if late_user else 0
        
        # Analyze if user is adapting toward influencer style
        word_adaptation = abs(late_avg_word - inf_avg_word) < abs(early_avg_word - inf_avg_word)
        punct_adaptation = abs(late_punct_rate - inf_punct_rate) < abs(early_punct_rate - inf_punct_rate)
        
        # Draw conclusions
        if word_adaptation and punct_adaptation:
            return "Strong linguistic accommodation - user is adapting to match influencer's style."
        elif word_adaptation or punct_adaptation:
            return "Moderate linguistic accommodation - some adaptation to influencer's style."
        else:
            return "Limited linguistic accommodation - user maintains distinct communication style."

    def save_session_memory(self):
        if self.custom_session_memory:
            session_file = os.path.join(self.memory_directory, f"session_{self.session_id}.json")
            with open(session_file, 'w') as f:
                json.dump(self.custom_session_memory, f, indent=2)
            if self.current_user_id:
                self.update_user_biography()
    
    def get_current_user_biography(self):
        if not self.current_user_id or self.current_user_id not in self.user_biographies:
            return ""
        bio = self.user_biographies[self.current_user_id]
        
        # Include trust history if available
        trust_info = ""
        if "trust_history" in bio and bio["trust_history"]:
            recent_trust = bio["trust_history"][-1]
            trust_info = f"\nTrust level: {recent_trust['avg_trust']:.2f}"
            
            # Compare with previous sessions if available
            if len(bio["trust_history"]) > 1:
                prev_trust = bio["trust_history"][-2]["avg_trust"]
                if recent_trust["avg_trust"] > prev_trust:
                    trust_info += " (increasing)"
                elif recent_trust["avg_trust"] < prev_trust:
                    trust_info += " (decreasing)"
                else:
                    trust_info += " (stable)"
        
        return f"USER BIOGRAPHY:\n{bio['biography']}\nInteractions: {bio['interaction_count']}\nFirst seen: {bio['first_seen']}\nLast updated: {bio['last_updated']}{trust_info}"

    def predict_response(self, conversation_history, bot_message):
        # Extract clean conversation history
        clean_history = self._extract_conversation_for_context(conversation_history)
        
        # Add learning from past prediction accuracy
        prediction_learning = self._generate_prediction_learning()
        
        # Add current conversation stage and trust metrics
        current_stage = conversation_memory.get("current_stage", "INITIAL_ENGAGEMENT")
        trust_metrics = self._get_current_trust_metrics()
        
        # Get user biography for context
        user_bio = self.get_current_user_biography()
        
        # Construct prompt
        prompt = f"""
Based on the following conversation history:
{self._format_conversation(clean_history)}

The influencer just said: {bot_message}

Current conversation stage: {current_stage}
{trust_metrics}

User Profile:
{user_bio}

{prediction_learning}

How would this specific user respond? Generate a realistic response that matches their communication style and likely concerns.
For this stage ({current_stage.lower().replace('_', ' ')}), consider:
1. The user's trust level and susceptibility 
2. Typical user behaviors during this stage
3. The user's demonstrated communication patterns
"""
        
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=prompt)
        ]
        
        response = self.llm.invoke(messages)
        return response.content

    def _extract_conversation_for_context(self, full_history):
        # Return only the actual user-influencer exchanges as a clean list
        clean_history = []
        for msg in conversation_memory["messages"]:
            clean_history.append(msg)
        return clean_history
        
    def _generate_prediction_learning(self):
        if not self.custom_session_memory or len(self.custom_session_memory) < 2:
            return ""
        
        # Get past predictions with actual responses
        learning_examples = []
        for entry in self.custom_session_memory:
            if entry["twin_prediction"] and entry["actual_response"]:
                learning_examples.append({
                    "predicted": entry["twin_prediction"],
                    "actual": entry["actual_response"]
                })
        
        # Use most recent examples
        learning_examples = learning_examples[-3:]
        
        if not learning_examples:
            return ""
        
        # Format learning prompt
        learning_text = "LEARNING FROM PAST PREDICTIONS:\n"
        for ex in learning_examples:
            learning_text += f"I predicted: {ex['predicted']}\n"
            learning_text += f"User actually said: {ex['actual']}\n\n"
        
        return learning_text

    def _format_conversation(self, messages):
        formatted = ""
        for msg in messages:
            formatted += f"{msg['role']}: {msg['content']}\n"
        return formatted

    def _get_current_trust_metrics(self):
        """Get current trust metrics for the digital twin's prediction context."""
        if not conversation_memory["trust_scores"]:
            return "Trust metrics: Not enough data yet."
        
        trust_scores = conversation_memory["trust_scores"]
        current_trust = trust_scores[-1]["score"]
        
        # Calculate trust trend
        trend = ""
        if len(trust_scores) >= 3:
            recent_scores = [entry["score"] for entry in trust_scores[-3:]]
            if recent_scores[2] > recent_scores[0]:
                trend = "increasing"
            elif recent_scores[2] < recent_scores[0]:
                trend = "decreasing"
            else:
                trend = "stable"
        
        # Categorize trust level
        trust_level = "low"
        if current_trust > 0.7:
            trust_level = "high"
        elif current_trust > 0.4:
            trust_level = "medium"
        
        return f"""Trust metrics:
- Current trust score: {current_trust:.2f}
- Trust level: {trust_level}
- Trust trend: {trend if trend else "insufficient data"}"""

# Initialize Digital Twin and set user session
digital_twin = DigitalTwinWithMemory(digital_twin_llm, DIGITAL_TWIN_SYSTEM_PROMPT)
DEFAULT_USER_ID = "demo_user"
digital_twin.set_user_for_session(DEFAULT_USER_ID)

#####################################
# 5. Helper: Extract Final Message  #
#####################################

def extract_final_message(full_text: str) -> str:
    match = re.search(r"<final_message>\s*(.*?)\s*</final_message>", full_text, flags=re.DOTALL)
    if match:
        return match.group(1).strip()
    else:
        return full_text.strip()

#####################################
# 5A. Helper: Dynamic Link Replacement
#####################################

def generate_contextual_link(final_message: str) -> str:
    words = re.sub(r'[^\w\s]', '', final_message).lower().split()
    stopwords = {'the', 'and', 'to', 'of', 'a', 'i', 'you', 'it', 'in', 'is', 'that', 'this', 'for', 'with', 'on'}
    keywords = [word for word in words if word not in stopwords]
    if keywords:
        slug = "-".join(keywords[:2])
        return f"http://www.example.com/{slug}"
    return "http://www.example.com/default"

def dynamic_link(final_message: str) -> str:
    if "[link]" in final_message:
         return final_message.replace("[link]", generate_contextual_link(final_message))
    return final_message

#####################################
# 5B. Helper: Safe Extract Final Response from Debug JSON
#####################################

def safe_extract_final_response(debug_json: str) -> str:
    try:
        data = json.loads(debug_json)
        return data.get("final_response", "")
    except Exception:
        return ""

#####################################
# 6. Helper: Check if Refinement is Needed
#####################################

def needs_refinement(initial: str, predicted: str, threshold: float = 0.5) -> bool:
    initial_words = set(initial.lower().split())
    predicted_words = set(predicted.lower().split())
    if not initial_words or not predicted_words:
        return False
    similarity = len(initial_words & predicted_words) / len(initial_words | predicted_words)
    return similarity < threshold

#####################################
# 7. Conversation Processing Function
#####################################

# Helper to get conversation history in message format for LLM
def get_conversation_messages():
    messages = []
    for msg in conversation_memory["messages"]:
        if msg["role"] == "USER":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "INFLUENCER":
            messages.append(AIMessage(content=msg["content"]))
    return messages

# Process message function completely rewritten to use JSON memory
def process_message(user_input):
    try:
        print(f"Processing message: '{user_input}'")
        
        # Get conversation history as messages for LLM
        context = get_conversation_messages()[-4:] if len(get_conversation_messages()) > 4 else get_conversation_messages()
        
        # Get current conversation stage
        current_stage = conversation_memory.get("current_stage", "INITIAL_ENGAGEMENT")
        stage_policy = CONVERSATION_STAGES[current_stage]["policy"]
        
        # Generate initial influencer response with stage-specific policy
        system_message = SystemMessage(content=f"{INFLUENCER_SYSTEM_PROMPT}\n\nCURRENT STAGE: {current_stage}\nSTAGE POLICY: {stage_policy}")
        initial_response = influencer_llm.invoke([system_message] + context + [HumanMessage(content=user_input)])
        print(f"Initial response generated: {initial_response.content[:50]}...")
        
        # Digital Twin prediction (feedback only)
        predicted_response = digital_twin.predict_response(conversation_memory["messages"], initial_response.content)
        print(f"Predicted user response: {predicted_response[:50]}...")
        
        # Save in digital twin memory
        context_data = [{"role": "USER", "content": user_input}]
        digital_twin.add_to_session_memory(context_data, predicted_response)
        
        # Refinement: instruct influencer to refine its message considering the stage
        refinement_prompt = f"""
You are the Influencer Agent.
User said: {user_input}
Initial response: {initial_response.content}
Digital Twin predicted: {predicted_response}

CURRENT CONVERSATION STAGE: {current_stage}
STAGE POLICY:
{stage_policy}

Based on this stage policy and the predicted user response, please refine your message.
Output ONLY the final user-facing text enclosed between <final_message> and </final_message> tags.
Do not include any additional commentary.
"""
        refinement_response = influencer_llm.invoke([
            SystemMessage(content=INFLUENCER_SYSTEM_PROMPT),
            HumanMessage(content=user_input),
            AIMessage(content=initial_response.content),
            SystemMessage(content=f"A typical user might respond: {predicted_response}"),
            HumanMessage(content=refinement_prompt)
        ])
        
        print(f"Refinement raw response: {refinement_response.content[:50]}...")
        raw_refinement = refinement_response.content
        
        if "KEEP ORIGINAL" in raw_refinement:
            final_message = initial_response.content
        else:
            final_message = extract_final_message(raw_refinement)
            
        # Replace placeholder with dynamic link based on context
        final_message = dynamic_link(final_message)
        
        # Store the final message in conversation memory
        conversation_memory["messages"].append({
            "role": "INFLUENCER",
            "content": final_message,
            "timestamp": datetime.datetime.now().isoformat()
        })
        
        # Update last prediction with influencer's final message
        if digital_twin.custom_session_memory:
            digital_twin.custom_session_memory[-1]["inf_prediction"] = final_message
        
        # Determine and update conversation stage
        next_stage = determine_next_stage(
            current_stage, 
            user_input, 
            final_message, 
            click_detected="http" in final_message and "http://" in final_message
        )
        update_conversation_stage(next_stage)
        
        # Update stage if changed
        if next_stage != current_stage:
            conversation_memory["current_stage"] = next_stage
            conversation_memory["stage_history"].append(next_stage)
            print(f"Stage transition: {current_stage} -> {next_stage}")
        
        return final_message
    except Exception as e:
        print(f"Error in process_message: {str(e)}")
        return f"I encountered an error while processing your message: {str(e)}"

def update_digital_twin_actual_response(actual_user_response):
    if digital_twin.custom_session_memory:
        last_entry = digital_twin.custom_session_memory[-1]
        if last_entry["actual_response"] is None:
            last_entry["actual_response"] = actual_user_response
            
        # Also update in the digital_twin_memory
        for pred in reversed(digital_twin_memory["predictions"]):
            if pred["actual"] is None:
                pred["actual"] = actual_user_response
                break

# Helper function to get trust score summary for UI display
def get_trust_score_summary():
    """Get a summary of trust scores for UI display."""
    if not conversation_memory["trust_scores"]:
        return "No trust data available yet."
        
    trust_scores = conversation_memory["trust_scores"]
    current_trust = trust_scores[-1]["score"]
    
    # Calculate overall metrics
    avg_trust = sum(item["score"] for item in trust_scores) / len(trust_scores)
    max_trust = max(item["score"] for item in trust_scores)
    min_trust = min(item["score"] for item in trust_scores)
    
    # Calculate trend
    trend = "stable"
    if len(trust_scores) >= 3:
        recent_scores = [item["score"] for item in trust_scores[-3:]]
        if recent_scores[2] > recent_scores[0] and recent_scores[2] > recent_scores[1]:
            trend = "increasing"
        elif recent_scores[2] < recent_scores[0] and recent_scores[2] < recent_scores[1]:
            trend = "decreasing"
    
    # Get the trust level category
    trust_level = "low"
    if current_trust > 0.7:
        trust_level = "high"
    elif current_trust > 0.4:
        trust_level = "medium"
        
    return f"""Trust Score Summary:
Current: {current_trust:.2f} ({trust_level})
Average: {avg_trust:.2f}
Range: {min_trust:.2f} - {max_trust:.2f}
Trend: {trend}
Stage: {conversation_memory["current_stage"]}"""

# Update function to display memory in UI
def refresh_memories():
    # Return data in a format similar to the original for UI compatibility
    influencer_mem = {
        "history": conversation_memory["messages"],
        "current_stage": conversation_memory["current_stage"],
        "stage_history": conversation_memory["stage_history"],
        "link_clicks": conversation_memory["link_clicks"],
        "trust_scores": conversation_memory["trust_scores"],
        "trust_summary": get_trust_score_summary() if conversation_memory["trust_scores"] else "No trust data yet"
    }
    digital_twin_mem = {
        "predictions": digital_twin_memory["predictions"],
        "custom_session_memory": digital_twin.custom_session_memory,
        "user_biography": digital_twin.get_current_user_biography()
    }
    return influencer_mem, digital_twin_mem

#####################################
# 8. New Function: Record Link Click (No End Message)
#####################################

def record_link_click(chat_history, state):
    event_text = "Link clicked"
    state["conv"].append(("LINK_CLICKED", event_text))
    chat_history.append(("Link Event", event_text))
    
    # Update conversation memory for link click
    conversation_memory["link_clicks"] += 1
    # Determine next stage after link click (should be SESSION_COMPLETION)
    next_stage = determine_next_stage(conversation_memory["current_stage"], "", "", click_detected=True)
    conversation_memory["current_stage"] = next_stage
    conversation_memory["stage_history"].append(next_stage)
    
    final_end_message = "Conversation ended. Please provide your feedback below."
    state["conv"].append(("SYSTEM", final_end_message))
    chat_history.append(("SYSTEM", final_end_message))
    save_conversation({"conv": state["conv"]})
    return chat_history, state, final_end_message, update_stage_display(next_stage)

#####################################
# 9. New Function: Record Feedback
#####################################

def record_feedback(feedback_text, chat_history, state):
    state["conv"].append(("FEEDBACK", feedback_text))
    chat_history.append(("Feedback", feedback_text))
    # Update to GOAL_COMPLETION stage after feedback
    conversation_memory["current_stage"] = "GOAL_COMPLETION"
    conversation_memory["stage_history"].append("GOAL_COMPLETION")
    save_conversation({"conv": state["conv"]})
    return chat_history, state, feedback_text, update_stage_display("GOAL_COMPLETION")

#####################################
# 10. Session Reset Function
#####################################

def reset_session():
    # Clear all memory structures
    conversation_memory["messages"] = []
    conversation_memory["current_stage"] = "INITIAL_ENGAGEMENT"
    conversation_memory["stage_history"] = []
    conversation_memory["link_clicks"] = 0
    conversation_memory["trust_scores"] = []
    conversation_memory["response_timestamps"] = []
    conversation_memory["engagement_depth"] = {
        "current_score": 0.5,
        "history": [],
        "substantive_count": 0
    }
    digital_twin_memory["predictions"] = []
    digital_twin.custom_session_memory = []
    return {"conv": []}, "Session reset.", update_stage_display("INITIAL_ENGAGEMENT")

#####################################
# 11. Gradio UI & Auto-Scrolling Setup
#####################################

STORAGE_DIR = "conversation_logs"
os.makedirs(STORAGE_DIR, exist_ok=True)

AUTO_SCROLL_JS = """
setTimeout(() => {
    // Attempt to find the chatbox by ID
    let chatContainer = document.getElementById('chatbox');
    // Fallback if not found by ID
    if (!chatContainer) {
        chatContainer = document.querySelector('.chatbox');
    }
    // Additional fallback: if the above still isn't found, try a typical Gradio chat class
    if (!chatContainer) {
        chatContainer = document.querySelector('.gradio-chatbot');
    }
    if (chatContainer) {
        chatContainer.scrollTop = chatContainer.scrollHeight;
    }
}, 300);
"""


def save_conversation(conversation_data, filename=None):
    if filename is None:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{STORAGE_DIR}/conversation_{timestamp}.json"
    with open(filename, 'w') as f:
        json.dump(conversation_data, f, indent=2)
    return filename

def add_user_message(user_message, chat_history, state):
    if not user_message.strip():
        return chat_history, state, user_message
    
    # Track timestamp for response time analysis
    track_response_timestamp()
    
    # Record actual user response for the previous prediction
    update_digital_twin_actual_response(user_message)
    
    # Add to conversation memory
    conversation_memory["messages"].append({
        "role": "USER",
        "content": user_message,
        "timestamp": datetime.datetime.now().isoformat()
    })
    
    # Update UI state
    state["conv"].append((f"USER: {user_message}", None))
    chat_history.append((user_message, "Thinking..."))
    
    return chat_history, state, user_message

def process_and_update(user_message, chat_history, state):
    if not user_message.strip():
        return chat_history, state, json.dumps({"status": "No message to process"}), update_stage_display(conversation_memory["current_stage"])
    try:
        response = process_message(user_message)
        
        # Update UI state
        for i in range(len(state["conv"]) - 1, -1, -1):
            if state["conv"][i][1] is None:
                state["conv"][i] = (state["conv"][i][0], response)
                break
        
        for i in range(len(chat_history) - 1, -1, -1):
            if chat_history[i][1] == "Thinking...":
                chat_history[i] = (chat_history[i][0], response)
                break
        
        debug_info = {
            "conversation_state": state["conv"],
            "chat_history": chat_history,
            "final_response": response
        }
        
        # Get current stage for UI update
        current_stage = conversation_memory["current_stage"]
        stage_display_html = update_stage_display(current_stage)
        
        return chat_history, state, json.dumps(debug_info, indent=2), stage_display_html
    except Exception as e:
        error_msg = f"Error: {str(e)}"
        print(f"DEBUG - Error in processing: {error_msg}")
        return chat_history, state, json.dumps({"error": error_msg}), update_stage_display(conversation_memory["current_stage"])

#####################################
# 12. Build the Gradio Interface
#####################################

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# Enhanced Two-Agent Persuasive System with Custom JSON Memory")
    gr.Markdown("This system uses a custom JSON-based memory system to manage conversation history and user biography across sessions.")
    
    # Add CSS for stage progress visualization
    gr.HTML("""
    <style>
    .stage-progress {
      position: relative;
      margin: 20px 0;
      height: 80px;
    }
    .stage-container {
      display: flex;
      justify-content: space-between;
      position: relative;
      z-index: 1;
    }
    .stage-item {
      display: flex;
      flex-direction: column;
      align-items: center;
      width: 18%;
      opacity: 0.5;
      transition: opacity 0.3s;
    }
    .stage-item.active {
      opacity: 1;
      font-weight: bold;
    }
    .stage-marker {
      width: 30px;
      height: 30px;
      border-radius: 50%;
      background: #ccc;
      color: #333;
      display: flex;
      align-items: center;
      justify-content: center;
      margin-bottom: 5px;
    }
    .stage-item.active .stage-marker {
      background: #2196F3;
      color: white;
    }
    .stage-label {
      font-size: 12px;
      text-align: center;
    }
    .progress-bar {
      position: absolute;
      height: 4px;
      background: #2196F3;
      top: 15px;
      left: 0;
      z-index: 0;
      transition: width 0.5s;
    }
    </style>
    """)
    
    # Add stage progress visualization
    with gr.Row():
        stage_display = gr.HTML(value=update_stage_display("INITIAL_ENGAGEMENT"), label="Conversation Stage")
    
    conversation_state = gr.State({"conv": []})
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(label="Conversation", elem_id="chatbox", height=400)
            with gr.Row():
                msg = gr.Textbox(label="Your Message", scale=3)
                send = gr.Button("Send", scale=1)
                # Link button initially disabled
                link_click = gr.Button("Record Link Click", scale=1, interactive=False)
                reset = gr.Button("Reset Session", scale=1)
        with gr.Column(scale=1):
            with gr.Tabs():
                with gr.TabItem("Debug Info"):
                    debug_output = gr.JSON(label="Debug Information")
                with gr.TabItem("Saved Logs"):
                    log_files = gr.Dropdown(label="Select Log File", choices=[f for f in os.listdir(STORAGE_DIR) if f.endswith('.json')])
                    refresh_btn = gr.Button("Refresh")
                    log_content = gr.JSON(label="Log Content")
                with gr.TabItem("Influencer Memory"):
                    influencer_memory_display = gr.JSON(label="Influencer Memory")
                with gr.TabItem("Digital Twin Memory"):
                    digital_twin_memory_display = gr.JSON(label="Digital Twin Memory")
    # Feedback textbox and submit button, initially hidden
    feedback = gr.Textbox(label="Feedback", placeholder="Enter your feedback here...", visible=False)
    submit_feedback = gr.Button("Submit Feedback", scale=1, visible=False)
    
    send.click(
        add_user_message,
        inputs=[msg, chatbot, conversation_state],
        outputs=[chatbot, conversation_state, msg]
    ).then(
        process_and_update,
        inputs=[msg, chatbot, conversation_state],
        outputs=[chatbot, conversation_state, debug_output, stage_display]
    ).then(
        lambda debug_json: gr.update(interactive=("http://" in safe_extract_final_response(debug_json))),
        outputs=[link_click]
    ).then(
        lambda: "",
        outputs=[msg]
    ).then(
        None, None, None, js=AUTO_SCROLL_JS  # <-- MUST be last in chain
    )

    
    msg.submit(
        add_user_message,
        inputs=[msg, chatbot, conversation_state],
        outputs=[chatbot, conversation_state, msg]
    ).then(
        process_and_update,
        inputs=[msg, chatbot, conversation_state],
        outputs=[chatbot, conversation_state, debug_output, stage_display]
    ).then(
        lambda debug_json: gr.update(interactive=("http://" in safe_extract_final_response(debug_json))),
        outputs=[link_click]
    ).then(
        lambda: "",
        outputs=[msg]
    ).then(
        None, None, None, js=AUTO_SCROLL_JS
    )
    
    reset.click(
        reset_session,
        inputs=[],
        outputs=[conversation_state, debug_output, stage_display]
    ).then(
        None, None, None, js=AUTO_SCROLL_JS
    )
    
    refresh_btn.click(lambda: gr.Dropdown(choices=[f for f in os.listdir(STORAGE_DIR) if f.endswith('.json')]),
                      outputs=[log_files])
    log_files.change(
        lambda f: json.load(open(os.path.join(STORAGE_DIR, f), 'r')) if f else {},
        inputs=[log_files],
        outputs=[log_content]
    )
    
    link_click.click(
        record_link_click,
        inputs=[chatbot, conversation_state],
        outputs=[chatbot, conversation_state, debug_output, stage_display]
    ).then(
        lambda: "",
        outputs=[msg]
    ).then(
        lambda: (gr.update(visible=True, value="Please enter your feedback about the conversation:"), gr.update(visible=True)),
        outputs=[feedback, submit_feedback]
    ).then(
        None, None, None, js=AUTO_SCROLL_JS
    )
    
    submit_feedback.click(
        record_feedback,
        inputs=[feedback, chatbot, conversation_state],
        outputs=[chatbot, conversation_state, debug_output, stage_display]
    ).then(
        None, None, None, js=AUTO_SCROLL_JS
    )
    
    refresh_mem_btn = gr.Button("Refresh Memory Views")
    refresh_mem_btn.click(
        refresh_memories,
        outputs=[influencer_memory_display, digital_twin_memory_display]
    )
    
    gr.Markdown("""
    1. The Influencer Agent generates a persuasive response using conversation context from the JSON memory.
    2. The Digital Twin predicts a realistic user response based on conversation history and stores predictions for long-term learning.
    3. A feedback loop refines the Influencer Agent's response, which is output between <final_message> tags.
    4. Only the final user-facing messages (user inputs and influencer responses) are stored in the conversation memory.
    5. The "Record Link Click" button is enabled only if the final message contains a dynamic link generated contextually.
       Clicking it logs the event, ends the conversation, persists the conversation, and reveals a feedback textbox and submit button.
    6. The Reset Session button clears all stored memory for a fresh session.
    7. Use the memory tabs to view clear, labeled conversation logs.
    """)
    
    # Add the stage progress visualization
    with gr.Row():
        stage_display = gr.HTML(
            """<div class="stage-progress">
                <div class="stage-container">"""
        )
    
    def update_stage_display(current_stage):
        """
        Generates HTML to display the current conversation stage as a progress bar.
        
        Args:
            current_stage (str): The current stage ID
            
        Returns:
            str: HTML string representing the progress bar
        """
        stages = [
            {"id": "INITIAL_ENGAGEMENT", "name": "Initial Engagement", "position": 1},
            {"id": "RAPPORT_BUILDING", "name": "Building Rapport", "position": 2},
            {"id": "TRUST_DEVELOPMENT", "name": "Developing Trust", "position": 3},
            {"id": "LINK_INTRODUCTION", "name": "Resource Sharing", "position": 4},
            {"id": "GOAL_COMPLETION", "name": "Completion", "position": 5}
        ]
        
        # Find current stage position
        current_position = 1
        progress_percentage = 0
        
        for stage in stages:
            if stage["id"] == current_stage:
                current_position = stage["position"]
                break
        
        # Calculate progress percentage (for the progress bar width)
        progress_percentage = ((current_position - 1) / (len(stages) - 1)) * 100
        
        # Generate HTML
        html = '<div class="stage-progress">'
        html += '<div class="progress-bar" style="width: {}%;"></div>'.format(progress_percentage)
        html += '<div class="stage-container">'
        
        for stage in stages:
            active_class = "active" if stage["id"] == current_stage else ""
            html += f'<div class="stage-item {active_class}">'
            html += f'<div class="stage-marker">{stage["position"]}</div>'
            html += f'<div class="stage-label">{stage["name"]}</div>'
            html += '</div>'
        
        html += '</div></div>'
        return html

# Helper function to determine next stage based on context
def determine_next_stage(current_stage, user_input, influencer_response, click_detected=False):
    """Determine the next conversation stage based on context, trust metrics, and conversation dynamics."""
    # Handle explicit stage transitions
    if click_detected:
        return "SESSION_COMPLETION"
    
    current = conversation_memory["current_stage"]
    messages_count = len(conversation_memory["messages"])
    
    # Get enhanced metrics
    engagement_depth = calculate_engagement_depth(user_input, conversation_memory["messages"])
    substantive_ratio = calculate_substantive_ratio(conversation_memory["messages"])
    word_count = len(user_input.split())
    
    # Calculate trust with enhanced algorithm
    trust_score = calculate_user_trust_score(user_input, influencer_response)
    
    # Store trust score for analysis
    conversation_memory["trust_scores"].append({
        "score": trust_score,
        "message_count": messages_count,
        "timestamp": datetime.datetime.now().isoformat(),
        "engagement_metrics": get_quality_metrics()
    })
    
    print(f"METRICS - Trust: {trust_score:.2f}, Engagement: {engagement_depth:.2f}, Substantive: {substantive_ratio:.2f}, Words: {word_count}")
    
    # Check for regression conditions - fallback to earlier stages if engagement drops
    if len(conversation_memory["engagement_depth"]["history"]) >= 3:
        # Get last three engagement scores
        recent_scores = conversation_memory["engagement_depth"]["history"][-3:]
        # If declining engagement pattern detected, consider regression
        if recent_scores[2] < recent_scores[0] and recent_scores[2] < recent_scores[1]:
            engagement_drop = recent_scores[0] - recent_scores[2]
            # Significant engagement drop triggers regression
            if engagement_drop > 0.3 and current not in ["INITIAL_ENGAGEMENT", "RAPPORT_BUILDING"]:
                print(f"Engagement regression detected: {engagement_drop:.2f}. Moving back a stage.")
                return previous_stage(current)
    
    # Stage transition logic with quality gates
    if current == "INITIAL_ENGAGEMENT":
        # Require at least 2 substantive exchanges
        if (messages_count >= 4 and engagement_depth > 0.5 and 
            word_count > 5 and substantive_ratio > 0.4 and trust_score > 0.3):
            print("Stage criteria met: INITIAL_ENGAGEMENT -> RAPPORT_BUILDING")
            return "RAPPORT_BUILDING"
            
    elif current == "RAPPORT_BUILDING":
        # Require sustained engagement and personal disclosure
        personal_disclosure = calculate_personal_disclosure(conversation_memory["messages"])
        if (messages_count >= 6 and engagement_depth > 0.6 and 
            personal_disclosure > 0.4 and trust_score > 0.5):
            print("Stage criteria met: RAPPORT_BUILDING -> TRUST_DEVELOPMENT")
            return "TRUST_DEVELOPMENT"
            
    elif current == "TRUST_DEVELOPMENT":
        # Require demonstrated interest in resources
        resource_interest = calculate_resource_interest(conversation_memory["messages"])
        if (messages_count >= 8 and engagement_depth > 0.65 and 
            resource_interest > 0.5 and trust_score > 0.6):
            print("Stage criteria met: TRUST_DEVELOPMENT -> LINK_INTRODUCTION")
            return "LINK_INTRODUCTION"
            
    elif current == "LINK_INTRODUCTION" and "http" in influencer_response:
        # Check for sufficient user consideration time
        if get_message_response_time() > 10 and trust_score > 0.6:
            print("Stage criteria met: LINK_INTRODUCTION -> LINK_REINFORCEMENT")
            return "LINK_REINFORCEMENT"
    
    # No change in stage
    return current

# Calculate user trust score based on research-backed metrics
def calculate_user_trust_score(user_input, influencer_response):
    """
    Implements ELM and Sequential Persuasion research to calculate a trust score 
    from user interactions, using a multi-metric fusion approach.
    """
    # Reduce weight for simple affirmations
    simple_affirmation_score = sum(1 for word in ["yes", "ok", "cool", "sure", "fine"] 
                                if word in user_input.lower()) * 0.05
    
    # Increase weight for substantive responses
    substantive_score = min(1.0, len(user_input.split()) / 20)  # Max score at 20 words
    
    # Message content analysis (self-disclosure, sentiment, etc.)
    sentiment_score = analyze_sentiment_and_disclosure(user_input) * 0.2
    
    # Engagement pattern analysis
    engagement_score = analyze_engagement_patterns() * 0.2
    
    # New engagement depth component
    depth_score = calculate_engagement_depth(user_input, conversation_memory["messages"]) * 0.3
    
    # Linguistic analysis with minimum length requirement
    linguistic_score = 0
    if len(user_input.split()) > 5:
        linguistic_score = analyze_linguistic_accommodation(user_input, influencer_response) * 0.2
    
    # Time factor - penalize very quick responses slightly
    time_since_last = get_message_response_time()
    time_factor = 1.0 - min(0.2, time_since_last/30)  # Slight penalty for very quick responses
    
    # Combine all factors
    combined_score = (
        simple_affirmation_score +
        (substantive_score * 0.3) +
        sentiment_score +
        (engagement_score * time_factor) +
        depth_score +
        linguistic_score
    )
    
    # Ensure score is in 0-1 range
    return max(0.0, min(1.0, combined_score))

if __name__ == "__main__":
    print(f"API Key exists: {os.environ.get('NEBIUS_API_KEY') is not None}")
    demo.launch()
